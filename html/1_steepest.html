<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Steepest descent</title>

    <link rel="icon" href="../favicon_32_32.png" sizes="32x32" type="image/png" />
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Math.js for Symbolic expressions -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/11.11.1/math.min.js"></script>

    <!-- Plotly.js for plotting -->
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js"></script>

    <!-- MathJax for rendering LaTeX math -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Merriweather font -->
    <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">

    <!-- Style file -->
    <link href="../style.css" rel="stylesheet" />

    <!-- JavaScript code -->
    <script src="../js/navbar.js"></script>
    <script type="module" src="../js/page.js"></script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F7L5N95HL3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-F7L5N95HL3');
    </script>
</head>

<!-- Main content and body of the page -->

<body class="bg-light px-4 py-2" data-page="steepest">

    <!-- Navbar -->
    <div id="navbar-container"></div>

    <!-- Input parameter -->
    <div class="container-fluid px-0 pt-2 pb-1">
        <form id="ParameterForm" class="bg-white rounded shadow px-4 py-2">
            <div class="row g-3 align-items-center justify-content-start">
                <!-- Toggle Switch -->
                <div id="toggle2D_container" class="col-12 col-md-auto d-flex align-items-center gap-2">
                </div>

                <!-- Objective Function -->
                <div id="objective_container" class="col-12 col-md-auto d-flex align-items-center gap-2">
                    <!-- To be filled dynamically -->
                </div>

                <!-- Line Search Strategy -->
                <div id="line_search_container" class="col-12 col-md-auto d-flex align-items-center gap-2">
                </div>

                <!-- Initial x₀ -->
                <div id="x0_container" class="col-12 col-md-auto d-flex align-items-center gap-2">
                </div>

                <!-- Initial y₀ -->
                <div id="y0_container" class="col-12 col-md-auto d-flex align-items-center gap-2">
                </div>

                <!-- Step Length -->
                <div id="step_length_container" class="col-12 col-md-auto d-flex align-items-center gap-2">
                </div>

                <!-- Calculate Button -->
                <div id="calculate_container" class="col-12 col-md-auto d-flex align-items-center">
                </div>
            </div>
        </form>
    </div>



    <div class="container-fluid px-0 py-1">
        <div class="row g-2 align-items-start-end">

            <!-- Plot Section -->
            <div class="col-12 col-lg-8">
                <div class="bg-white rounded shadow p-3 h-100 d-flex justify-content-center align-items-center">
                    <div id="plot" class="w-100" style="height: 500px"></div>
                </div>
            </div>

            <!-- Iteration Box -->
            <div class="col-12 col-lg-4">
                <div class="bg-white rounded shadow p-3 h-100 overflow-auto"
                    style="font-family: monospace; font-size: 14px; max-height: 550px; overflow-y: auto;">
                    <h6 id="termination"></h6>
                    <table class="table mb-0">
                        <thead class="sticky-top bg-white">
                            <tr>
                                <th scope="col">Iteration</th>
                                <th scope="col">x</th>
                                <th scope="col">y</th>
                                <th scope="col">f(x, y)</th>
                            </tr>
                        </thead>
                        <tbody id="optTableBody">
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>

    <!-- Documentation Section -->

    <div class="container mx-5, my-5">
        <h1 class="fw-bold mb-3 text-primary">Steepest Descent Method</h1>

        <p>
            The <strong>Steepest Descent</strong> (also known as <em>Gradient Descent</em>) method is one of the simplest \(1^{st} \) order optimization algorithms. In order to find the optimal solution of a function, this method calculates the first derivative of the function. 
        </p>

        <h4 class="fw-bold mb-3 text-primary">1. Method</h4>
        <img src="../docs/sd_1.png" alt="Descriptive Alt" class="img-fluid float-end me-3 m-3 rounded shadow"
            style="width: 500px;">

        <p>
            Consider a function \(f: \mathbb{R}^n \to \mathbb{R}\) that is differentiable, and let \(x_0\) be the
            initial guess.
            The steepest descent method iteratively updates the prior point.
            For the \(1^{st}\) iteration the update is done using the formula:
        </p>
        \[
        x_{1} = x_0 + t_0 \ d_0
        \]

        <p>
            where \(t_0\) is the step size (or learning rate) and \( d_0 = -\nabla f(x_0) \) is the descent direction, which is negative of the gradient of the function.
            
        </p>
        
        <p>
            For \((k+1)^{th}\) iteration, the update rule becomes:
            \[
            x_{k+1} = x_{k} + t_k \ d_k
            \]
            where \(t_k\) and \(d_k\) are the step size and descent direction at iteration \(k\), respectively. The descent direction \( d_k \) is given by : \( d_k = -\nabla f(x_k)\). Note, all the optimization algorithms differ in the way they calculate the step length and descent direction.
        </p>

        <p>
            The iterative process continues until predefined convergence criteria is met. Typical convergence criteria include: If change in function values between iterations is less than a predefined threshold \(\varepsilon\), change in the input value between iterations is small, or if the gradient is close to zero, i.e., \(|\nabla f(x_k)| < \varepsilon\).

        </p>
        <p>
            The step size \(t_k\) can be determined using various methods, including:
        <ul>
            <li>Constant step size</li>
            <li>Armijo Rule</li>
            <li>Exact line search</li>
        </ul>


        <h5 class="fw-bold mb-3 text-primary">1.2 Line search method</h5>
        <p> The step length \(t_k\) is determined using a line search method, which aims to find the optimal step
            size that minimizes the function along the direction of the gradient.</p>
        <p> The most common line search methods include:
        <h6>Constant step length</h6>
        <p> In this method, a fixed step length is used for all iterations. This is simple but may not be optimal for
            all iterations.</p>
        <h6>Armijo rule</h6>
        <p> The Armijo rule is a backtracking line search method that adjusts the step length based on the decrease in
            the function value. It ensures that the step length is not too large, which could lead to overshooting the
            minimum.</p>
        <h6>Exact line search</h6>
        <p> The exact line search method finds the optimal step length by minimizing the function along the direction of
            the gradient. This is computationally expensive but can lead to faster convergence.</p>

        <p> The choice of line search method can significantly affect the convergence rate of the steepest descent
            method.</p>
        </p>

        <h4 class="fw-bold mb-3 text-primary">2. Algorithm</h4>
        <p>
            The algorithm for the steepest descent method can be summarized as follows:
        <ol>
            <li>Initialize \(x_0\) and set a tolerance level \(\varepsilon\).</li>
            <li>Compute the gradient \(\nabla f(x_k)\).</li>
            <li>Compute the descent direction \(d_k = - \nabla f(x_k)\).</li>
            <li>Determine the step length \(t_k\) using method of choice (constant step size, Armijo rule, or exact line search)</li>
            <li>Update the point: \(x_{k+1} = x_k + t_k \ d_k\).</li>
            <li>Check for convergence:
                <ul>
                    <li>If \(|\nabla f(x_k)| < \varepsilon\), <strong>exit the loop</strong></li>
                    <li>Otherwise  \(k = k + 1\), go to step 2.</li>
                </ul>
        </ol>
        </p>

        <h4 class="fw-bold mb-3 text-primary">3. Example</h4>
        <p>
            Consider the function \(f(x) = x^2 + 2x + 1\). The gradient is given by \(\nabla f(x) = 2x + 2\).
            Starting from an initial guess \(x_1 = 4\), we can apply the steepest descent method with a constant step
            size of \(t = 0.1\).
            The iterations would look like this:

            \[
            \text{Iteration 1 : } x_1 = x_0 - t \cdot \nabla f(x_0) = 4 - 0.1 \cdot (2 \cdot 4 + 2) = 3
            \]
            \[
            \text{Iteration 2 : } x_2 = x_1 - t \cdot \nabla f(x_1) = 3 - 0.1 \cdot (2 \cdot 3 + 2) = 2.2
            \]

            and iterative process continues, until convergence condition, \(|f(x_{k+1}) - f(x_k)| < \varepsilon\) or, \(|x_{k+1} - x_k| < \varepsilon\) or \(|\nabla f(x_k)| < \varepsilon\), is met.

        </p>


        <h4 class="fw-bold mb-3 text-primary">4. Discussion</h4>
        <p>
            The steepest descent method is a simple and effective 1st derivative optimization algorithm. For differentiable functions, it always converges to a local minimum under appropriate choice of step length. However, the choice of step length and line search method can
            significantly affect the convergence rate.
        </p>
        <!-- <h4 class="fw-bold mb-3 text-primary">6. References</h4> -->
    </div>



</body>

</html>